# Проанализировать трафик компьютерной сети на основе дистрибутивов Linux

используется 3 вмки - 2 клиента 1 сервер
## analise.py - для анализа файлов pcap - просто путь вставить

На вмках предварительно должно быть 
1. обновить, добавить ансибл, добавить подключение между всеми по ключу так sh-keygen ssh-copy-id ub2@192.168.0.132 
1. добавить на хостовую машину все другие машины (hosts) /etc/ansible/hosts
2. проверить что все ок ansible -m ping all -u ansible -k

Запуск ансибла `sudo ansible-playbook /etc/ansible/app.yml -kK`

# case 1
## сценарий для iperf3-сервера типа один сервер и два клиента
Установить все что нужно 
/etc/ansible/iperf.yml - тут установка + запуск нагрузки и тестирваонеи как разделить сервера на группы запустить iperf3-сервер на одном хосте и генерировать трафик на клиентах

После выполнения, файлы с расширением `.pcap` можно анализировать. 


## подробное описание, сгенерированное гпт
Этот плейбук Ansible выполняет автоматизированную установку и запуск инструментов для анализа сетевого трафика, а также генерацию и захват сетевого трафика между группами серверов и клиентов. Вот подробное описание шагов:

1. **Установка инструментов анализа трафика**  
   На всех хостах (и серверах, и клиентах) устанавливаются пакеты `iperf3` (для генерации/измерения сетевого трафика) и `tcpdump` (для захвата сетевых пакетов).

2. **Запуск iperf3-сервера**  
   На всех хостах из группы `server_group` запускается iperf3 в режиме сервера (`iperf3 -s`) в фоне. Это позволяет принимать подключения от клиентов для тестирования пропускной способности сети.

3. **Генерация трафика и захват пакетов**  
   На каждом хосте из группы `client_group` (по одному за раз, благодаря `serial: 1`):
   - Запускается iperf3 в режиме клиента, который подклкакторый захватывает весь сетевой трафик на интерфейсах клиента и сохраняет его в файл `/tmp/traffic_dump_<имя_клиента>.pcap` (захват длится 20 секунд).
   - После завершения захвата файл с дампом трафика пересылается на сервер в папку `/tmp/logs/` с помощью `scp`.

**Итог:**  
Плейбук позволяет автоматически развернуть инструменты для анализа сети, провести тестирование пропускной способности между клиентами и сервером, а также собрать дампы сетевого трафика для последующего анализа. Всё это делается последовательно для каждого клиента, чтобы избежать конфликтов при одновременных подключениях к iperf3-серверу.

потом этот файл скопировать и открыть его 

# case 2
## анализ с приложением на 5000 порту 

пререквесты (по идее можно и в ансибл запихать но мне лень)
1. запускам билд docker build -t app .
2. пробрасывааем порт docker run -d --name app -p 5000:5000 app
3. проверить в ручную чт о приложение запущено и рабоатте curl http://localhost:5000/ curl http://192.168.0.213:5000/ (оно на сервере такой и порт)
4. запустить ансибл  sudo ansible-playbook /etc/ansible/app.yml -kK
5. Посмотреть куда сохранились логи загнать логи в приложение для анализа вайршарк или куда угодно

Описание от гпт 
Файл app.yml — это Ansible playbook для автоматизации тестирования и анализа сетевого трафика приложения, работающего на порту 5000. Вот что он делает по шагам:

1. **Устанавливает зависимости**: Устанавливает пакеты `tcpdump` (для захвата трафика) и `apache2-utils` (для генерации нагрузки через `ab`).
2. **Останавливает старые процессы tcpdump**: Завершает все ранее запущенные процессы захвата трафика на порту 5000.
3. **Создаёт директорию для файлов захвата**: Убеждается, что директория tmp существует.
4. **Запускает захват трафика**: Запускает `tcpdump` для записи трафика на порт 5000 в файл на каждом узле.
5. **Делает паузу**: Ждёт несколько секунд, чтобы захват трафика начался до генерации нагрузки.
6. **Генерирует нагрузку**: На клиентах запускает утилиту `ab` для отправки запросов к серверу на порт 5000.
7. **Проверяет наличие файла захвата**: Убеждается, что файл с трафиком создан.
8. **Скачивает файл дампа**: Копирует файл с трафиком на управляющую машину в папку `/tmp/logs`.
9. **Удаляет файл дампа с узла**: Очищает временные файлы на удалённых машинах.
10. **Показывает путь к файлу дампа**: Выводит сообщение с путём к скачанному файлу трафика.

Таким образом, playbook автоматизирует процесс тестирования производительности приложения и сбора сетевого трафика для последующего анализа.

Потом все логи собираются на контрольной машине 

# case 3
## кидать  туда сюда файлы и запросы подняв но одном из серверов nginx и тоже прочекать трафик
1. docker build -t nginx_server .
2. пробросим порты  docker run -d --name nginx_server -p 8080:80 nginx_server
3. Проверить доступность 
4. Запустить отправку запросов и обмен файлами из плейбукка files.yml

Этот плейбук выполняет автоматизированное тестирование сетевого взаимодействия между группами серверов и клиентов, а также собирает сетевые логи для последующего анализа. Вот что он делает по шагам:

1. **Устанавливает утилиту tcpdump** на всех целевых машинах для захвата сетевого трафика.
2. **Запускает tcpdump в фоновом режиме** на всех машинах, чтобы начать запись сетевого трафика в файл.
3. **Создаёт директорию для тестовых файлов** с открытыми правами доступа на всех машинах.
4. **С клиентов отправляет HTTP-запросы на сервер**, чтобы сгенерировать сетевой трафик.
5. **Создаёт тестовые файлы на клиентах** и копирует их на сервер с помощью scp, чтобы проверить передачу файлов по сети.
6. **Создаёт несколько тестовых файлов на клиентах** и копирует их на сервер, а затем один из файлов копирует обратно с сервера на клиента, чтобы проверить двустороннюю передачу файлов.
7. **Останавливает tcpdump** на всех машинах.
8. **Проверяет наличие файла с логами трафика** и скачивает его с каждой машины на управляющий узел для дальнейшего анализа.

В результате плейбук позволяет проверить сетевое взаимодействие между машинами и собрать полные логи сетевого трафика для диагностики и анализа.

### Пример анализа трафика для файла "logs/logs/files_192.168.0.213.pcap"

Total packets: 350805

Top 5 IP addresses:
192.168.0.213: 796 packets
192.168.0.103: 546 packets
10.42.0.1: 349 packets
10.42.0.111: 241 packets
192.168.0.37: 227 packets

Protocol usage:
TCP: 1279 packets
UDP: 1 packets

Top 5 ports:
Port 22: 745 packets
Port 49002: 315 packets
Port 10250: 229 packets
Port 6444: 96 packets
Port 8181: 60 packets

--- DETAILED NETWORK ANALYSIS ---

TCP Sessions: 86
  Complete sessions: 5
  Incomplete sessions: 41
  Reset sessions: 4

Packet Loss Indicators:
  TCP Retransmissions: 462
  Out-of-order packets: 21

Round-Trip Time (RTT) Statistics:
  Minimum RTT: 0.000000 seconds
  Average RTT: 0.056926 seconds
  Maximum RTT: 0.189035 seconds

Estimated TCP packet loss: 36.12%
  Lost packets (retransmissions): 462
  Successfully delivered packets: 817

Average Packet Size: 22.27 bytes

Top 5 IP pairs:
192.168.0.103 -> 192.168.0.213: 271 packets
192.168.0.213 -> 192.168.0.103: 271 packets
10.42.0.1 -> 10.42.0.111: 120 packets
192.168.0.37 -> 192.168.0.213: 120 packets
127.0.0.1 -> 127.0.0.1: 111 packets

Top 5 TTL values:
TTL 64: 1248 packets
TTL 63: 32 packets